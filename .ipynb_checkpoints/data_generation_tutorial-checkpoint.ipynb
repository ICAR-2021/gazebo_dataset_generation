{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbb8f84",
   "metadata": {},
   "source": [
    "# Automatic Dataset Generation From CAD for Vision-Based Grasping\n",
    "\n",
    "This script demonstrates the generation of a synthetic dataset of a multi-object scenario using CAD files.\n",
    "\n",
    "The dataset consists of the followinf data :\n",
    "\n",
    "- RGB images \n",
    "- Depth Images\n",
    "- Pose information\n",
    "- Binary Mask \n",
    "\n",
    "---\n",
    "Please include the following citation of our [paper](https://ieeexplore.ieee.org/abstract/document/9659336), if you use this work in your publications\n",
    "\n",
    "```\n",
    "@INPROCEEDINGS{9659336,\n",
    "  author={Ahmad, Saad and Samarawickrama, Kulunu and Rahtu, Esa and Pieters, Roel},\n",
    "  booktitle={2021 20th International Conference on Advanced Robotics (ICAR)}, \n",
    "  title={Automatic Dataset Generation From CAD for Vision-Based Grasping}, \n",
    "  year={2021},\n",
    "  volume={},\n",
    "  number={},\n",
    "  pages={715-721},\n",
    "  doi={10.1109/ICAR53236.2021.9659336}}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382a124",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "Please make sure to follow the [installation](https://github.com/KulunuOS/gazebo_dataset_generation#installation) from the github repository before running the notebook. Remember to change your jupyter notebook kernel to \"open3d\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9bd5fd",
   "metadata": {},
   "source": [
    "### 1. Initialization\n",
    "\n",
    " 1. Run the gazebo world in a new terminal `$ roslaunch data_generation metrics.launch` \n",
    " 2. Import the required modules by running the code block below \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646d33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import rospy\n",
    "import tf\n",
    "from cv_bridge import CvBridge, CvBridgeError\n",
    "from gazebo_msgs.msg import LinkState as stateGZ\n",
    "from gazebo_msgs.srv import GetModelState as getStateGZ\n",
    "from gazebo_msgs.srv import SetLinkState as setStateGZ\n",
    "from geometry_msgs.msg import (Point, Pose, PoseArray, PoseStamped, Quaternion,\n",
    "                               Twist, Vector3)\n",
    "from rospy.exceptions import ROSException\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from sensor_msgs.msg import CameraInfo, Image, PointCloud2\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import cv2\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import argparse\n",
    "import ipywidgets as wgt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44272384",
   "metadata": {},
   "source": [
    " 3. Create the directories if they donot already exist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbde25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Create the directories\n",
    "#rospy.signal_shutdown(\"done \")\n",
    "if not os.path.exists('rgb'):\n",
    "    os.makedirs('rgb')\n",
    "if not os.path.exists('depth'):\n",
    "    os.makedirs('depth')\n",
    "if not os.path.exists('meta'):\n",
    "    os.makedirs('meta')\n",
    "if not os.path.exists('mask'):\n",
    "    os.makedirs('mask')\n",
    "if not os.path.exists('model_pointcloud'):\n",
    "    os.makedirs('model_pointcloud') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01321da",
   "metadata": {},
   "source": [
    " 4. Lets pass the two CAD models `bottom_casing` and `left_gear` as arguments to our python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778947e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Argument parser\n",
    "parser = argparse.ArgumentParser(description = 'parse some parameters')\n",
    "parser.add_argument(\"models\", nargs='+', help=\"Enter the names of the models seperated by a space\")\n",
    "#args = parser.parse_args()\n",
    "args = parser.parse_args('bottom_casing left_gear'.split())\n",
    "#args = parser.parse_args('left_gear bottom_casing'.split())\n",
    "n_models = len(args.models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e41d55e",
   "metadata": {},
   "source": [
    "5. Initialize the required ros functionalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b86d8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rospy.init_node('data_render_gazebo', anonymous = True)\n",
    "rate = rospy.Rate(0.5)\n",
    "bridge = CvBridge()\n",
    "cam_info_msg = rospy.wait_for_message('kinect1/color/camera_info', CameraInfo, timeout = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99410e4",
   "metadata": {},
   "source": [
    "### 2. Setting camera pose in gazebo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf5c5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Function to set camera position and pose\n",
    "def set_cam_state_gazebo(camPos, camTwist):\n",
    "    # Set cam state in gazebo\n",
    "    camstate = stateGZ('kinect_ros::link', camPos, camTwist, 'world' )\n",
    "    print('Transforming camera to pose : '+str(sample_num))\n",
    "    try:\n",
    "       gzclient = rospy.ServiceProxy('gazebo/set_link_state', setStateGZ)\n",
    "       resp = gzclient(camstate)\n",
    "        \n",
    "    except Exception as inst:\n",
    "           print('Error in gazebo/set_link_state service request: ' + str(inst) )\n",
    "              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cf2abe",
   "metadata": {},
   "source": [
    " - Explanation on Pose and Twist message\n",
    " - Explanation on Eular vs Quaternion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e956f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_params(phi,theta,dist):\n",
    "    theta_rad = np.deg2rad(theta)\n",
    "    phi_rad = np.deg2rad(phi)\n",
    "    X = dist*np.cos(phi_rad)*np.cos(theta_rad)\n",
    "    Y = dist*np.cos(phi_rad)*np.sin(theta_rad)\n",
    "    Z = np.abs(dist*np.sin(phi_rad)) + 0.84\n",
    "\n",
    "    cam_euler = R.from_euler('xyz',[0,phi,theta+180], degrees=True)\n",
    "    cam_quat = cam_euler.as_quat()\n",
    "    \n",
    "    camPos = Pose(position= Point(x=X, y=Y, z=Z), \n",
    "                  orientation= Quaternion(x=cam_quat[0], y=cam_quat[1] , z=cam_quat[2], w=cam_quat[3]))\n",
    "    camTwist = Twist(linear= Vector3(x=0, y=0, z=0) , \n",
    "                     angular= Vector3(x=0, y=0, z=0))\n",
    "    \n",
    "    return camPos,camTwist, X, Y,Z, cam_euler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174ef3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cam_state(phi,theta,dist):\n",
    "     \n",
    "    camPos,camTwist,_,_,_,_ = calc_params(phi,theta,dist)\n",
    "    set_cam_state_gazebo(camPos, camTwist)\n",
    "    \n",
    "    frame = rospy.wait_for_message('/kinect1/color/image_raw', Image, timeout = 3)\n",
    "    frame =  bridge.imgmsg_to_cv2(frame, desired_encoding='rgb8')\n",
    "    imshow(frame)\n",
    "    \n",
    "    return \n",
    "\n",
    "sample_num = 'test_pose'    \n",
    "wgt.interact(test_cam_state,\n",
    "             phi=wgt.IntSlider(min=0, max=90, step=1, value=45),\n",
    "             theta=wgt.IntSlider(min=0, max=360, step=1, value=45),\n",
    "             dist= wgt.FloatSlider(min=0, max=1, step=0.01, value=0.25));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037468e2",
   "metadata": {},
   "outputs": [],
   "source": [
    " #%% Function to convert between Image types for depth images\n",
    "def convert_types(img, orig_min, orig_max, tgt_min, tgt_max, tgt_type):\n",
    "\n",
    "    #info = np.finfo(img.dtype) # Get the information of the incoming image type\n",
    "    # normalize the data to 0 - 1\n",
    "    img_out = img / (orig_max-orig_min)   # Normalize by input range\n",
    "    img_out = (tgt_max - tgt_min) * img_out # Now scale by the output range\n",
    "    img_out = img_out.astype(tgt_type)\n",
    "\n",
    "    #cv2.imshow(\"Window\", img)\n",
    "    return img_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d32c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_dup():\n",
    "    rgb_duplicate = True                     \n",
    "    while rgb_duplicate:\n",
    "        print('Subscribing to rgb topics...')\n",
    "        img_msg = rospy.wait_for_message('/kinect1/color/image_raw', Image, timeout = 3)\n",
    "        cv_image = bridge.imgmsg_to_cv2(img_msg, desired_encoding='rgb8')\n",
    "        #cv_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if sample_num > 0:  \n",
    "            #No point checking for the sample 0\n",
    "            previous_im = cv2.imread('rgb/'+str(sample_num-1)+'.png', -1)\n",
    "            rgb_duplicate = abs(np.mean(cv_image - previous_im)) < 2 # Mean of all pixels shouldn't be this small if it's two different images\n",
    "            print('rgb diff: '+str(np.mean(cv_image - previous_im)))\n",
    "            print(rgb_duplicate)\n",
    "            if rgb_duplicate:\n",
    "                #Try setting state again. Sometimes gazebo trips out as well.\n",
    "                set_cam_state_gazebo(camPos, camTwist)\n",
    "\n",
    "        else:\n",
    "            rgb_duplicate = False\n",
    "\n",
    "    depth_duplicate = True \n",
    "    while depth_duplicate:\n",
    "        print('Subscribing to depth topics...')\n",
    "        depthImg_msg = rospy.wait_for_message('/kinect1/depth/image_raw', Image, timeout = 3 )\n",
    "        cv_depthImage = bridge.imgmsg_to_cv2(depthImg_msg, desired_encoding='passthrough')\n",
    "        if sample_num > 0:\n",
    "            previous_im = cv2.imread('depth/'+str(sample_num-1)+'.png', -1)\n",
    "            depth_duplicate = abs(np.nanmean(cv_depthImage - previous_im))< 200  # Mean of all pixels shouldn't be this small if it's two different images\n",
    "            print('depth diff: '+str(np.nanmean(cv_depthImage - previous_im)))# - previous_im)))\n",
    "            print(depth_duplicate)\n",
    "            if depth_duplicate:\n",
    "                #Try setting state again. Sometimes gazebo trips out as well.\n",
    "                set_cam_state_gazebo(camPos, camTwist)\n",
    "        else:\n",
    "            depth_duplicate = False\n",
    "          \n",
    "    return cv_image, cv_depthImage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aca960f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Camera Extrinsics\n",
    "def get_camera_extrinsics(phi,theta, dist):\n",
    "    \n",
    "    _,_,X,Y,Z,_ = calc_params(phi,theta,dist)\n",
    "\n",
    "    cam_euler = R.from_euler('xyz',[0,phi,theta+180], degrees=True)\n",
    "    cam_world_R = cam_euler.as_matrix()\n",
    "    cam_world_t = np.array([X,Y,Z]).reshape(3,1)\n",
    "    cam_world_T = np.hstack((cam_world_R,cam_world_t))\n",
    "    cam_world_T = np.vstack((cam_world_T, [0,0,0,1]))\n",
    "    \n",
    "    return cam_world_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f64f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get object states and save pose\n",
    "def get_object_states(n_models):\n",
    "    resp=[]\n",
    "    #Get object state \n",
    "    try: \n",
    "        rospy.wait_for_service('gazebo/get_model_state')\n",
    "        client = rospy.ServiceProxy('gazebo/get_model_state', getStateGZ)\n",
    "        for i in range(n_models):\n",
    "            #print(args.models[i])\n",
    "            #print(client(args.models[i], 'world'))\n",
    "            resp.append( client(args.models[i], 'world'))\n",
    "    except Exception as inst:\n",
    "        print('Error in gazebo/get_link_state service request: ' + str(inst) )\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995b2869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Object pose in world frame obtained from Gazebo Service\n",
    "def get_object2cam_pose(resp, n_models, cam_world_T):\n",
    "    obj_cam_T = np.zeros((  4, 4, n_models )) # Transformation Mats for 10 object classes\n",
    "    \n",
    "    for i in range(0 , n_models):\n",
    "        obj_pos = np.array([resp[i].pose.position.x, resp[i].pose.position.y, resp[i].pose.position.z]).reshape(3,1)\n",
    "        obj_or = [resp[i].pose.orientation.x, resp[i].pose.orientation.y, resp[i].pose.orientation.z, resp[i].pose.orientation.w]\n",
    "        obj_or = (R.from_quat(obj_or)).as_matrix()\n",
    "        obj_world_T = np.concatenate((obj_or, obj_pos), axis = 1) \n",
    "\n",
    "        # Transformation from object2world to  object2cam for GT label poses\n",
    "        #obj_cam_T = np.dot(obj_world_T, np.linalg.inv(cam_world_T) )\n",
    "        obj_world_T = np.vstack(( obj_world_T, [0,0,0,1] ))    \n",
    "        obj_cam_T[:, :, i] = np.dot( np.linalg.inv(cam_world_T), obj_world_T )#[:3,:]\n",
    "    \n",
    "    gt_dict = { 'poses':obj_cam_T[:3,:,:] } #returns [ R  T , i] \n",
    "    return  gt_dict, obj_cam_T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa06f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_object2cam_pose(n_models,phi,theta, dist):\n",
    "    resp = get_object_states(n_models)\n",
    "    cam_world_T = get_camera_extrinsics(phi,theta, dist)\n",
    "    gt_dict, obj_cam_T = get_object2cam_pose(resp, n_models, cam_world_T)\n",
    "    \n",
    "    return gt_dict, obj_cam_T\n",
    "    \n",
    "\n",
    "gt_dict, obj_cam_T = test_object2cam_pose(n_models,phi=45,theta=45, dist=0.25)\n",
    "print(obj_cam_T)\n",
    "sio.savemat('meta/'+str(sample_num)+'-meta.mat',gt_dict)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598f16b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Load the meshes of all objects convert them to point clouds\n",
    "# combine and return the pointclouds of all meshes in a dictionary\n",
    "\n",
    "def mesh2pcld(n_models):\n",
    "    all_points = {}\n",
    "    all_pclds  = {}\n",
    "\n",
    "    for i in range (0,n_models):\n",
    "        mesh = o3d.io.read_triangle_mesh('data_generation/models/'+str(args.models[i])+'/meshes/'+str(args.models[i])+'.STL')\n",
    "        poisson_pcld = mesh.sample_points_poisson_disk(number_of_points=30000) \n",
    "        all_pclds[str(args.models[i])] = poisson_pcld\n",
    "        o3d.io.write_point_cloud('model_pointcloud/'+str(args.models[i])+'.ply', poisson_pcld )\n",
    "        all_points[str(args.models[i])] = np.asarray(poisson_pcld.points)#, dtype= np.float32)\n",
    "        \n",
    "    return all_points, all_pclds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20fb60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_mesh2pcld(n_models):\n",
    "    all_points, all_pclds = mesh2pcld(n_models)\n",
    "    for i in args.models: \n",
    "        assert i in all_points.keys()\n",
    "        assert len(all_points[str(i)]) == 30000   \n",
    "        \n",
    "        \n",
    "    o3d.visualization.draw_geometries([all_pclds[str(args.models[1])]])\n",
    "\n",
    "test_mesh2pcld(n_models)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc082c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_im_fill(bin_im):\n",
    "    ### https://www.learnopencv.com/filling-holes-in-an-image-using-opencv-python-c/ ###\n",
    "\n",
    "    # Copy the binary image.\n",
    "    im_floodfill = bin_im.copy()\n",
    "\n",
    "    # Mask used to flood filling.\n",
    "    # Notice the size needs to be 2 pixels smaller than the image.\n",
    "    h, w = bin_im.shape[:2]\n",
    "    mask = np.zeros((h+2, w+2), np.uint8)\n",
    "\n",
    "    # Floodfill from point (0, 0)\n",
    "    cv2.floodFill(im_floodfill, mask, (0,0), 255);\n",
    "\n",
    "    # Invert floodfilled image\n",
    "    im_floodfill_inv = cv2.bitwise_not(im_floodfill)\n",
    "\n",
    "    # Combine the two images to get the foreground.\n",
    "    im_out = bin_im | im_floodfill_inv\n",
    "\n",
    "    return im_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78507fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the pointclouds to binary mask\n",
    "def pcl_2_binary_mask(obj_cam_T,n_models, all_points):\n",
    "    \n",
    "    #Projection Matrix / Camera instrinsics\n",
    "    cam_P = np.array(cam_info_msg.P).reshape(3,4)\n",
    "    cam_P = np.vstack((cam_P , [0,0,0,1]))\n",
    "    bin_mask = np.zeros((cam_info_msg.height, cam_info_msg.width), dtype= np.uint8)\n",
    "    mask_list = []\n",
    "    pixels_list = []\n",
    "    \n",
    "    #Camera optical link \n",
    "    cam2optical = R.from_euler('zyx',[1.57, 0, 1.57])\n",
    "    cam2optical = cam2optical.as_matrix()\n",
    "    op2cam_T = np.hstack(( np.vstack(( cam2optical , [0,0,0] )) , np.array([[0],[0],[0],[1]]) ))\n",
    "    \n",
    "    \n",
    "    for i in np.argsort(-obj_cam_T[2,3,:]):\n",
    "        print(str(args.models[i]))\n",
    "        print(i)\n",
    "        # copy all  the points\n",
    "        cloud_temp = copy.deepcopy(all_points[str(args.models[i])]).transpose().astype(np.float32)\n",
    "        cloud_temp = np.vstack(( cloud_temp, np.ones((1,cloud_temp.shape[1])) )).astype(np.float32)\n",
    "        \n",
    "        # Then transform it into camera's coordinate system\n",
    "        cloud_cam = np.dot(  obj_cam_T[:, :, i]  , cloud_temp).astype(np.float32)\n",
    "        \n",
    "        # transform from camera-link to camera optical link\n",
    "        cloud_optical = np.dot(op2cam_T, cloud_cam).astype(np.float32)\n",
    "        \n",
    "        # perspective projection into image-plane\n",
    "        x,y,z,w = np.dot( cam_P, cloud_optical ).astype(np.float32) #This is the projection step\n",
    "        print(z)\n",
    "        x = x / z\n",
    "        y = y / z\n",
    "        \n",
    "        print(x)\n",
    "\n",
    "        #clips out all the points projected out of image height and width\n",
    "        clipping = np.logical_and( np.logical_and(x>=0, x<=640) , np.logical_and(y>=0, y<=480) )\n",
    "        x = x[np.where(clipping)]\n",
    "        y = y[np.where(clipping)]\n",
    "        \n",
    "        #print(np.shape(x))\n",
    "        \n",
    "        #Leave the background black\n",
    "        pixels = np.vstack((x,y)).transpose()\n",
    "        pixels = np.array(pixels, dtype=np.uint16)\n",
    "        #print(np.shape(pixels))\n",
    "        pixels_list.append([pixels])\n",
    "        \n",
    "        this_mask = np.zeros((cam_info_msg.height, cam_info_msg.width), dtype= np.uint8)\n",
    "        \n",
    "        for point in pixels:\n",
    "            this_mask[point[1]-1, point[0]-1] = 255\n",
    "        \n",
    "        this_mask = cv_im_fill(this_mask)\n",
    "        \n",
    "        this_mask[this_mask.nonzero()] = 1.05*np.ceil(255*(i+1)/n_models)\n",
    "        r,c = this_mask.nonzero()\n",
    "        #print(np.unique(this_mask[r,c]))\n",
    "        #mask_list.append(this_mask)\n",
    "        \n",
    "        bin_mask[this_mask.nonzero()] = 0\n",
    "        bin_mask += this_mask\n",
    "        \n",
    "        \n",
    "    return mask_list, bin_mask,pixels_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dbc0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "camPos,camTwist, X, Y,Z, cam_euler = calc_params(45,45,0.25)\n",
    "set_cam_state_gazebo(camPos, camTwist)\n",
    "cam_world_T = get_camera_extrinsics(45,45,0.25)\n",
    "resp = get_object_states(n_models)\n",
    "_,obj_cam_T = get_object2cam_pose(resp, n_models, cam_world_T)\n",
    "all_points,_ = mesh2pcld(n_models)\n",
    "mask_list, bin_mask,pixels_list = pcl_2_binary_mask(obj_cam_T, n_models, all_points)\n",
    "\n",
    "#print(pixels_list[1])\n",
    "imshow(bin_mask, cmap='gray')\n",
    "r,c = bin_mask.nonzero()\n",
    "#print(np.unique(bin_mask[r,c]))\n",
    "\n",
    "#cv2.imwrite('mask/'+str(sample_num)+'.png',bin_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ec24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main loop\n",
    "sample_num = 0\n",
    "for dist in np.arange(0.15,0.5,0.125):\n",
    "    for phi in range(35,90,15):\n",
    "        for theta in range(0, 360, 15):\n",
    "            camPos,camTwist,_,_,_,_ =  calc_params(phi,theta,dist)\n",
    "            set_cam_state_gazebo(camPos, camTwist)\n",
    "            \n",
    "            \n",
    "            while not rospy.is_shutdown():\n",
    "                print('Subscribing to camera topics...')\n",
    "                try:\n",
    "                    cv_image, cv_depthImage = check_dup()\n",
    "                    break  \n",
    "                except ROSException as e:\n",
    "                        print('Timeout occured in subscribing.Trying again...')\n",
    "                        continue\n",
    "            \n",
    "            cv_depthImage = convert_types(cv_depthImage,0,3, 0,65535, np.uint16) ## 0 - 3m is the input range of kinect depth\n",
    "            print('Writing Images')\n",
    "            cv2.imwrite('rgb/'+str(sample_num)+'.png', cv_image)\n",
    "            cv2.imwrite('depth/'+str(sample_num)+'.png',cv_depthImage)\n",
    "                    \n",
    "            try:\n",
    "                resp = get_object_states(n_models)\n",
    "            except Exception as inst:\n",
    "                     print('Error in gazebo/get_link_state service request: ' + str(inst) )\n",
    "                \n",
    "            cam_world_T = get_camera_extrinsics(phi,theta,dist)\n",
    "            gt_dict,obj_cam_T = get_object2cam_pose(resp, n_models,cam_world_T)\n",
    "            \n",
    "            sio.savemat('meta/'+str(sample_num)+'-meta.mat',gt_dict)\n",
    "        \n",
    "            mask_list, bin_mask,pixels_list = pcl_2_binary_mask(obj_cam_T, n_models, all_points)\n",
    "            \n",
    "            cv2.imwrite('mask/'+str(sample_num)+'.png',bin_mask)\n",
    "            \n",
    "            sample_num += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1def48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacddef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64688bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c0ae00",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "open3d",
   "language": "python",
   "name": "open3d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
